spring.datasource.url=${UDS_JDBC_URL}
spring.datasource.username=${UDS_USERNAME}
spring.datasource.password=${UDS_PASSWORD}

spring.jpa.properties.hibernate.dialect = org.hibernate.dialect.PostgreSQLDialect
spring.jpa.properties.hibernate.default_schema=${UDS_SCHEMA}
spring.jpa.properties.hibernate.temp.use_jdbc_metadata_defaults = false

spring.jpa.properties.hibernate.jdbc.batch_size = 500
spring.jpa.properties.hibernate.order_inserts = true
spring.jpa.properties.hibernate.order_updates = true

spring.jpa.database-platform=org.hibernate.dialect.PostgreSQL9Dialect
spring.jpa.hibernate.ddl-auto = update
spring.jpa.show_sql=false
spring.datasource.hikari.maximum-pool-size=2
spring.cache.type=caffeine
spring.cache.caffeine.spec=maximumSize=10000,expireAfterWrite=1d

rabbitmq.queue.length=100000
rabbitmq.max.batches=5
rabbitmq.max.intransit=1

core.rabbitmq.listen_queues = hl7Queue,databaseExtracts,extensionProjects,waveform
# Data older than this is liable to be deleted to keep overall disk usage small.
# In production we will want to have this longer (more like 7 days)
core.waveform.retention_hours = 1
core.waveform.is_non_current_test_data = 0

spring.rabbitmq.listener.simple.acknowledge-mode=manual
